2018-04-26 15:58:13 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: YFspider2)
2018-04-26 15:58:13 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'YFspider2.spiders', 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'SPIDER_MODULES': ['YFspider2.spiders'], 'REACTOR_THREADPOOL_MAXSIZE': '32', 'BOT_NAME': 'YFspider2', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'logs\\default\\AtcOrgAu\\90353b40492711e887e20862667c7ee1.log'}
2018-04-26 15:58:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-04-26 15:58:13 [AtcOrgAu] INFO: Reading start URLs from redis key 'AtcOrgAu:start_urls' (batch size: 16, encoding: utf-8
2018-04-26 15:58:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-04-26 15:58:13 [py.warnings] WARNING: D:\python_27\lib\site-packages\scrapy\utils\deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.spidermiddleware.referer.RefererMiddleware` class is deprecated, use `scrapy.spidermiddlewares.referer.RefererMiddleware` instead
  ScrapyDeprecationWarning)

2018-04-26 15:58:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-04-26 15:58:13 [scrapy.middleware] INFO: Enabled item pipelines:
['scrapy_redis.pipelines.RedisPipeline',
 'YFspider2.pipelines.YfspidersetdefaultValue',
 'YFspider2.pipelines.save_data_to_RemoteFile_XMX',
 'YFspider2.pipelines.save_data_to_file']
2018-04-26 15:58:13 [scrapy.core.engine] INFO: Spider opened
2018-04-26 15:58:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-04-26 15:58:13 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2018-04-26 15:58:13 [AtcOrgAu] DEBUG: Read 1 requests from 'AtcOrgAu:start_urls'
2018-04-26 15:59:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-04-26 15:59:19 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.atc.org.au/> from <GET http://www.atc.org.au/>
2018-04-26 16:00:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-04-26 16:00:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.atc.org.au/> (referer: None)
2018-04-26 16:00:30 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "D:\python_27\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "D:\python_27\lib\site-packages\scrapy\crawler.py", line 285, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "D:\python_27\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "D:\python_27\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "D:\python_27\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "D:\python_27\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "D:\python_27\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "D:\python_27\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "D:\python_27\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "D:\python_27\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "D:\python_27\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "D:\python_27\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "D:\python_27\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "D:\python_27\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "D:\python_27\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "D:\python_27\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

2018-04-26 16:00:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.atc.org.au/> (referer: None)
Traceback (most recent call last):
  File "D:\python_27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
2018-04-26 16:00:30 [twisted] CRITICAL: Unhandled error in Deferred:
2018-04-26 16:00:30 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\python_27\lib\site-packages\twisted\internet\task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "D:\python_27\lib\site-packages\scrapy\utils\defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "D:\python_27\lib\site-packages\scrapy\core\scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "D:\python_27\lib\site-packages\scrapy\core\engine.py", line 210, in crawl
    self.schedule(request, spider)
  File "D:\python_27\lib\site-packages\scrapy\core\engine.py", line 216, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "D:\python_27\lib\site-packages\scrapy_redis\scheduler.py", line 162, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "D:\python_27\lib\site-packages\scrapy_redis\dupefilter.py", line 100, in request_seen
    added = self.server.sadd(self.key, fp)
  File "D:\python_27\lib\site-packages\redis\client.py", line 1600, in sadd
    return self.execute_command('SADD', name, *values)
  File "D:\python_27\lib\site-packages\redis\client.py", line 668, in execute_command
    return self.parse_response(connection, command_name, **options)
  File "D:\python_27\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "D:\python_27\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
2018-04-26 16:00:30 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "D:\python_27\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
    self.crawler_process.start()
  File "D:\python_27\lib\site-packages\scrapy\crawler.py", line 285, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "D:\python_27\lib\site-packages\twisted\internet\base.py", line 1243, in run
    self.mainLoop()
  File "D:\python_27\lib\site-packages\twisted\internet\base.py", line 1252, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "D:\python_27\lib\site-packages\twisted\internet\base.py", line 878, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "D:\python_27\lib\site-packages\scrapy\utils\reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "D:\python_27\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "D:\python_27\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "D:\python_27\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "D:\python_27\lib\site-packages\scrapy_redis\queue.py", line 115, in pop
    results, count = pipe.execute()
  File "D:\python_27\lib\site-packages\redis\client.py", line 2879, in execute
    return execute(conn, stack, raise_on_error)
  File "D:\python_27\lib\site-packages\redis\client.py", line 2775, in _execute_transaction
    self.immediate_execute_command('DISCARD')
  File "D:\python_27\lib\site-packages\redis\client.py", line 2715, in immediate_execute_command
    return self.parse_response(conn, command_name, **options)
  File "D:\python_27\lib\site-packages\redis\client.py", line 2838, in parse_response
    self, connection, command_name, **options)
  File "D:\python_27\lib\site-packages\redis\client.py", line 680, in parse_response
    response = connection.read_response()
  File "D:\python_27\lib\site-packages\redis\connection.py", line 629, in read_response
    raise response
redis.exceptions.ResponseError: DISCARD without MULTI

